# -*- coding: utf-8 -*-
"""base_model_train_resnet_cifar.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1guYsfT03gLUOccuagi0Jm2wvh-dMhSac
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

#!pip install tensorflow_datasets

import tensorflow as tf

from tensorflow.keras.applications import resnet
from tensorflow.keras.applications.resnet import preprocess_input
import matplotlib.pyplot as plt
import numpy as np
import tensorflow_datasets as tfds
physical_devices = tf.config.experimental.list_physical_devices('GPU')
assert len(physical_devices) > 0, "Not enough GPU hardware devices available"
#tf.config.experimental.set_memory_growth(physical_devices[0], True)
#tf.config.experimental.set_memory_growth(physical_devices[1], True)


#used to fix bug in keras preprocessing scope
temp = tf.zeros([4, 32, 32, 3])  # Or tf.zeros
preprocess_input(temp)
print("processed")

IMAGE_SIZE = (32, 32)
TRAIN_SIZE = 50000
VALIDATION_SIZE = 10000
BATCH_SIZE_PER_GPU = 128
global_batch_size = (BATCH_SIZE_PER_GPU * 1)
NUM_CLASSES = 10

"""Dataset code

# Augmentation Code
"""

def flip(x: tf.Tensor) -> tf.Tensor:
    """Flip augmentation

    Args:
        x: Image to flip

    Returns:
        Augmented image
    """
    x = tf.image.random_flip_left_right(x)
    #x = tf.image.random_flip_up_down(x)

    return x

def color(x: tf.Tensor) -> tf.Tensor:
    """Color augmentation

    Args:
        x: Image

    Returns:
        Augmented imageweights
    """
    x = tf.image.random_hue(x, 0.08)
    x = tf.image.random_saturation(x, 0.6, 1.6)
    x = tf.image.random_brightness(x, 0.05)
    x = tf.image.random_contrast(x, 0.7, 1.3)
    return x

def rotate(x: tf.Tensor) -> tf.Tensor:
    """Rotation augmentation

    Args:
        x: Image

    Returns:
        Augmented image
    """

    return tf.image.rot90(x, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))

def zoom(x: tf.Tensor) -> tf.Tensor:
    """Zoom augmentation

    Args:
        x: Image

    Returns:
        Augmented image
    """

    # Generate 20 crop settings, ranging from a 1% to 20% crop.
    scales = list(np.arange(0.8, 1.0, 0.01))
    boxes = np.zeros((len(scales), 4))

    for i, scale in enumerate(scales):
        x1 = y1 = 0.5 - (0.5 * scale)
        x2 = y2 = 0.5 + (0.5 * scale)
        boxes[i] = [x1, y1, x2, y2]

    def random_crop(img):
        # Create different crops for an image
        crops = tf.image.crop_and_resize([img], boxes=boxes, box_indices=np.zeros(len(scales)), crop_size=IMAGE_SIZE)
        # Return a random crop
        return crops[tf.random.uniform(shape=[], minval=0, maxval=len(scales), dtype=tf.int32)]


    choice = tf.random.uniform(())

    # Only apply cropping 50% of the time
    return tf.cond(choice < 0.5, lambda: x, lambda: random_crop(x))

def normalize(input_image):
  return preprocess_input(input_image)

@tf.function
def load_image_train(datapoint):
  input_image, label = tf.image.resize(datapoint["image"], IMAGE_SIZE), datapoint['label']
  # if tf.random.uniform(()) > 0.5:
  #   input_image = tf.image.flip_left_right(input_image)
  augmentations = [flip, color, zoom, rotate]
  for f in augmentations:
    input_image = tf.cond(tf.random.uniform(()) > 0.75, lambda: f(input_image), lambda: input_image)

  #input_image = preprocess_input(input_image)
  input_image = normalize(input_image)

  return input_image, tf.one_hot(label, depth=NUM_CLASSES)

@tf.function
def load_image_test(datapoint):
  input_image, label = tf.image.resize(datapoint["image"], IMAGE_SIZE), datapoint['label']
  #input_image = preprocess_input(input_image)

  input_image = normalize(input_image)

  return input_image, tf.one_hot(label, depth=NUM_CLASSES)

dataset, info = tfds.load('cifar10', with_info=True)

for example in dataset['train'].take(1):
  print(example["label"])

info

"""# Make the upscaled cifar dataset"""

train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)
train_dataset = train.shuffle(buffer_size=1000).batch(global_batch_size).repeat()
train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)


test_dataset = dataset['test'].map(load_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_dataset = test_dataset.batch(global_batch_size).repeat()
test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

"""# New Section"""

base_model = resnet.ResNet50(weights="imagenet", include_top=False)
alpha=1e-4
regularizer = tf.keras.regularizers.l2(alpha)

for layer in base_model.layers:
    for attr in ['kernel_regularizer']:
        if hasattr(layer, attr):
          setattr(layer, attr, regularizer)


def lr_schedule(epoch):
  lr = 1e-3
  if epoch > 180:
    lr *= 0.5e-3
  elif epoch > 160:
    lr *= 1e-3
  elif epoch > 120:
    lr *= 1e-2
  elif epoch > 80:
    lr *= 1e-1
  print('Learning rate: ', lr)
  return lr

x = base_model.output
x = tf.keras.layers.GlobalAveragePooling2D()(x)
#x = tf.keras.layers.Dropout(.3)(x)
x = tf.keras.layers.Dense(10)(x)
outputs = tf.keras.layers.Activation('softmax', dtype='float32', name='predictions')(x)
checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='cifar10.h5', verbose=1,monitor='val_acc', save_best_only=True)
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=np.sqrt(0.1), patience=5, min_lr=0.5e-6)
callbacks = [checkpoint, lr_scheduler, reduce_lr]
model = tf.keras.models.Model(inputs=base_model.input, outputs=outputs)

TRAIN_SIZE  // global_batch_size

VALIDATION_SIZE // global_batch_size

"""4/ugH8tvEReB5cvlCO8fqv4nxdyHVdwKynhQ0_RhOl1RIo2ZhGirH5mlM"""
import math
model.compile(tf.keras.optimizers.Adam(lr_schedule(0)), loss="categorical_crossentropy", metrics=['acc'])
model.fit(train_dataset,
          epochs=200,
          steps_per_epoch=math.ceil(TRAIN_SIZE/global_batch_size),
          validation_data=test_dataset,
          validation_steps=math.ceil(10000/global_batch_size),
          callbacks=callbacks)

"""# New Section"""

model.evaluate(test_dataset, steps=math.ceil(VALIDATION_SIZE/global_batch_size))

model.save('base_model_cifar10_resnet50.h5')

