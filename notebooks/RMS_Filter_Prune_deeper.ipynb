{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras import Model, Input\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, BatchNormalization, InputLayer, Layer\n",
    "\n",
    "#used to fix bug in keras preprocessing scope\n",
    "temp = tf.zeros([4, 32, 32, 3])  # Or tf.zeros\n",
    "preprocess_input(temp)\n",
    "print(\"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (64, 64)\n",
    "TRAIN_SIZE = 50000\n",
    "VALIDATION_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def normalize(input_image):\n",
    "  return preprocess_input(input_image)\n",
    "\n",
    "@tf.function\n",
    "def load_image_test(datapoint):\n",
    "  input_image, label = tf.image.resize(datapoint[\"image\"], IMAGE_SIZE), datapoint['label']\n",
    "  #input_image = preprocess_input(input_image)\n",
    "\n",
    "  input_image = normalize(input_image)\n",
    "\n",
    "  return input_image, tf.one_hot(label, depth=NUM_CLASSES)\n",
    "\n",
    "dataset, info = tfds.load('cifar10', with_info=True)\n",
    "train = dataset['train'].map(load_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_dataset = dataset['test'].map(load_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('/tf/notebooks/cifar10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_names = [ \"conv4_block2_2_relu\", \"conv4_block2_3_conv\"]\n",
    "outputs = []\n",
    "for layer in model.layers:\n",
    "    if layer.name in output_layer_names:\n",
    "        outputs.append(layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = Model(inputs=model.input, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "output_model.save('/tmp/output_model.h5')\n",
    "del model\n",
    "tf.keras.backend.clear_session()\n",
    "output_model = tf.keras.models.load_model('/tmp/output_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_layers = [\n",
    "                \"conv4_block2_2_conv\",\n",
    "                \"conv4_block2_2_bn\",\n",
    "                \"conv4_block2_2_relu\",\n",
    "                \"conv4_block2_3_conv\"\n",
    "                ]\n",
    "layer_dict = {}\n",
    "for layer in output_model.layers:\n",
    "    if layer.name in prune_layers:\n",
    "        layer_dict[layer.name] = {\"weights\":layer.get_weights()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 4, 4, 256) (256, 4, 4, 1024)\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in train_dataset.take(1):\n",
    "    x, y = output_model(x_batch)\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_config = output_model.get_config()\n",
    "for layer in prune_layers:\n",
    "    for layer_config in model_config['layers']:\n",
    "        if layer_config['config']['name'] == layer:\n",
    "            layer_dict[layer]['config'] =  layer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(4, 4, 256))\n",
    "layer_class = getattr(tf.keras.layers, layer_dict[prune_layers[0]]['config']['class_name'])\n",
    "x = layer_class.from_config(layer_dict[prune_layers[0]]['config']['config'])(input_tensor)\n",
    "x_0 = None\n",
    "for layer in prune_layers[1::]:\n",
    "    layer_class = getattr(tf.keras.layers, layer_dict[layer]['config']['class_name'])\n",
    "\n",
    "    x = layer_class.from_config(layer_dict[layer]['config']['config'])(x)\n",
    "\n",
    "sub_model = Model(inputs=input_tensor, outputs=[ x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 4, 4, 256)]       0         \n",
      "_________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D) (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNorm (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv4_block2_2_relu (Activat (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D) (None, 4, 4, 1024)        263168    \n",
      "=================================================================\n",
      "Total params: 854,272\n",
      "Trainable params: 853,760\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sub_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse_metric_0 = tf.keras.metrics.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in prune_layers:\n",
    "    for rep_layer in sub_model.layers:\n",
    "        if layer == rep_layer.name:\n",
    "            rep_layer.set_weights(layer_dict[layer]['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def calculate_mse(mse_metric_0, mse, train_dataset, output_model, sub_model):\n",
    "\n",
    "    for  (x_batch, y_batch) in train_dataset:\n",
    "\n",
    "        x, y_0= output_model(x_batch)\n",
    "        pre_0 = sub_model(x)\n",
    "        mse_metric_0.update_state(y_0, pre_0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fd8b00446a0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd8b0044320>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fd8b004b470>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7fd86404fd30>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fd85c7db630>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 31min 30s, sys: 7min 33s, total: 1h 39min 3s\n",
      "Wall time: 20min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "# save original weights\n",
    "og_conv_weights = sub_model.layers[1].get_weights()\n",
    "og_bn_weights = sub_model.layers[2].get_weights()\n",
    "\n",
    "filter_scores = []\n",
    "#prune and score each filter by MSE error\n",
    "for filters in range(256):\n",
    "    \n",
    "    # get inital weights\n",
    "    conv_weights = sub_model.layers[1].get_weights()\n",
    "    bn_weights = sub_model.layers[2].get_weights()\n",
    "    \n",
    "    #prun conv layer\n",
    "    conv_weights[0][:,:,:,filters] = np.zeros(conv_weights[0][:,:,:,filters].shape)\n",
    "    conv_weights[1][filters] = 0\n",
    "    \n",
    "    #prune batch norm variables\n",
    "    for weight in bn_weights:\n",
    "        weight[filters] = 0\n",
    "    \n",
    "    # reassign into submodel\n",
    "    sub_model.layers[1].set_weights(conv_weights)\n",
    "    sub_model.layers[2].set_weights(bn_weights)\n",
    "    \n",
    "    #calculate MSE over dataset and show result\n",
    "    calculate_mse(mse_metric_0,  mse, train_dataset, output_model, sub_model)\n",
    "    l2_norm = np.linalg.norm(og_conv_weights[0][:,:,:,filters])\n",
    "    result_0 = mse_metric_0.result().numpy()\n",
    "    filter_scores.append({'filter': filters, 'mse': result_0, 'L2': l2_norm})\n",
    "    mse_metric_0.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../cifar10.h5')\n",
    "m = tf.keras.metrics.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x_batch, y_batch) in test_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loss 1.6843410730361938, Accuracy: 0.9035999774932861'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "mse_sorted_filters = sorted(filter_scores, key=lambda x: x['mse'])\n",
    "mse_prune_filters = [x['filter'] for x in mse_sorted_filters[:math.floor(64*.95)]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_conv_weights = copy.deepcopy(og_conv_weights)\n",
    "mse_bn_weights = copy.deepcopy(og_bn_weights)\n",
    "for index in mse_prune_filters:\n",
    "    mse_conv_weights[0][:,:,:, index] = np.zeros(conv_weights[0][:,:,:,index].shape)\n",
    "    mse_conv_weights[1][index] = 0\n",
    "    \n",
    "    for weight in mse_bn_weights:\n",
    "        weight[index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.002582801505923271, Accuracy: 1.0\n",
      "Loss 1.7206816673278809, Accuracy: 0.9010000228881836\n"
     ]
    }
   ],
   "source": [
    "m.reset_states()\n",
    "acc.reset_states()\n",
    "\n",
    "for layer in model.layers:\n",
    "    if layer.name == \"conv4_block2_2_conv\":\n",
    "        layer.set_weights(mse_conv_weights)\n",
    "    elif layer.name == \"conv4_block2_2_bn\":\n",
    "        layer.set_weights(mse_bn_weights)\n",
    "\n",
    "for (x_batch, y_batch) in train_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)\n",
    "\n",
    "print(f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\")\n",
    "m.reset_states()\n",
    "acc.reset_states()\n",
    "\n",
    "for (x_batch, y_batch) in test_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)\n",
    "print(f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\")\n",
    "\n",
    "model.load_weights(\"../cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "l2_sorted_filters = sorted(filter_scores, key=lambda x: x['L2'])\n",
    "l2_prune_filters = [x['filter'] for x in l2_sorted_filters[:math.floor(64*.95)]]\n",
    "\n",
    "l2_conv_weights = copy.deepcopy(og_conv_weights)\n",
    "l2_bn_weights = copy.deepcopy(og_bn_weights)\n",
    "for index in l2_prune_filters:\n",
    "    l2_conv_weights[0][:,:,:, index] = np.zeros(conv_weights[0][:,:,:,index].shape)\n",
    "    l2_conv_weights[1][index] = 0\n",
    "    \n",
    "    for weight in l2_bn_weights:\n",
    "        weight[index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0021090859081596136, Accuracy: 1.0\n",
      "Loss 1.7103685140609741, Accuracy: 0.901199996471405\n"
     ]
    }
   ],
   "source": [
    "m.reset_states()\n",
    "acc.reset_states()\n",
    "\n",
    "for layer in model.layers:\n",
    "    if layer.name == \"conv4_block2_2_conv\":\n",
    "        layer.set_weights(l2_conv_weights)\n",
    "    elif layer.name == \"conv4_block2_2_bn\":\n",
    "        layer.set_weights(l2_bn_weights)\n",
    "\n",
    "\n",
    "for (x_batch, y_batch) in train_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)\n",
    "\n",
    "print(f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\")\n",
    "m.reset_states()\n",
    "acc.reset_states()\n",
    "\n",
    "for (x_batch, y_batch) in test_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)\n",
    "print(f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
