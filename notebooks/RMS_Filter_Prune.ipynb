{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.applications.resnet import preprocess_input\n",
    "from tensorflow.keras import Model, Input\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for device in physical_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, BatchNormalization, InputLayer, Layer\n",
    "\n",
    "#used to fix bug in keras preprocessing scope\n",
    "temp = tf.zeros([4, 32, 32, 3])  # Or tf.zeros\n",
    "preprocess_input(temp)\n",
    "print(\"processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (64, 64)\n",
    "TRAIN_SIZE = 50000\n",
    "VALIDATION_SIZE = 10000\n",
    "BATCH_SIZE = 256\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def normalize(input_image):\n",
    "  return preprocess_input(input_image)\n",
    "\n",
    "@tf.function\n",
    "def load_image_test(datapoint):\n",
    "  input_image, label = tf.image.resize(datapoint[\"image\"], IMAGE_SIZE), datapoint['label']\n",
    "  #input_image = preprocess_input(input_image)\n",
    "\n",
    "  input_image = normalize(input_image)\n",
    "\n",
    "  return input_image, tf.one_hot(label, depth=NUM_CLASSES)\n",
    "\n",
    "dataset, info = tfds.load('cifar10', with_info=True)\n",
    "train = dataset['train'].map(load_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_dataset = dataset['test'].map(load_image_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model('/tf/notebooks/cifar10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_names = [\"pool1_pool\", \"conv2_block1_2_relu\", \"conv2_block1_3_bn\"]\n",
    "outputs = []\n",
    "for layer in model.layers:\n",
    "    if layer.name in output_layer_names:\n",
    "        outputs.append(layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model = Model(inputs=model.input, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "conv1_conv (Conv2D)          (None, None, None, 64)    9472      \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)    (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)    (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D) (None, None, None, 64)    4160      \n",
      "_________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNorm (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2_block1_1_relu (Activat (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D) (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNorm (None, None, None, 64)    256       \n",
      "_________________________________________________________________\n",
      "conv2_block1_2_relu (Activat (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D) (None, None, None, 256)   16640     \n",
      "_________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNorm (None, None, None, 256)   1024      \n",
      "=================================================================\n",
      "Total params: 68,992\n",
      "Trainable params: 68,096\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "output_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "output_model.save('/tmp/output_model.h5')\n",
    "del model\n",
    "tf.keras.backend.clear_session()\n",
    "output_model = tf.keras.models.load_model('/tmp/output_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_layers = [\"conv2_block1_1_conv\", \n",
    "                \"conv2_block1_1_bn\", \n",
    "                \"conv2_block1_1_relu\",\n",
    "                \"conv2_block1_2_conv\",\n",
    "                \"conv2_block1_2_bn\",\n",
    "                \"conv2_block1_2_relu\",\n",
    "                \"conv2_block1_3_conv\",\n",
    "                \"conv2_block1_3_bn\"]\n",
    "layer_dict = {}\n",
    "for layer in output_model.layers:\n",
    "    if layer.name in prune_layers:\n",
    "        layer_dict[layer.name] = {\"weights\":layer.get_weights()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 16, 16, 64) (256, 16, 16, 64) (256, 16, 16, 256)\n"
     ]
    }
   ],
   "source": [
    "for x_batch, y_batch in train_dataset.take(1):\n",
    "    x, y_1, y_2 = output_model(x_batch)\n",
    "    print(x.shape, y_1.shape, y_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_config = output_model.get_config()\n",
    "for layer in prune_layers:\n",
    "    for layer_config in model_config['layers']:\n",
    "        if layer_config['config']['name'] == layer:\n",
    "            layer_dict[layer]['config'] =  layer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(16, 16, 64))\n",
    "layer_class = getattr(tf.keras.layers, layer_dict[prune_layers[0]]['config']['class_name'])\n",
    "x = layer_class.from_config(layer_dict[prune_layers[0]]['config']['config'])(input_tensor)\n",
    "x_0 = None\n",
    "for layer in prune_layers[1::]:\n",
    "    layer_class = getattr(tf.keras.layers, layer_dict[layer]['config']['class_name'])\n",
    "    if layer == 'conv2_block1_2_relu':\n",
    "        x_0 = layer_class.from_config(layer_dict[layer]['config']['config'])(x)\n",
    "    else:\n",
    "        x = layer_class.from_config(layer_dict[layer]['config']['config'])(x)\n",
    "\n",
    "sub_model = Model(inputs=input_tensor, outputs=[x_0, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 16, 16, 64)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 16, 16, 64)   4160        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 16, 16, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 16, 16, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 16, 16, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 16, 16, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 59,264\n",
      "Trainable params: 58,496\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sub_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "mse_metric_0 = tf.keras.metrics.MeanSquaredError()\n",
    "mse_metric_1 = tf.keras.metrics.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in prune_layers:\n",
    "    for rep_layer in sub_model.layers:\n",
    "        if layer == rep_layer.name:\n",
    "            rep_layer.set_weights(layer_dict[layer]['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def calculate_mse(mse_metric_0, mse_metric_1, mse, train_dataset, output_model, sub_model):\n",
    "\n",
    "    for  (x_batch, y_batch) in train_dataset:\n",
    "\n",
    "        x, y_0, y_1 = output_model(x_batch)\n",
    "        pre_0, pre_1 = sub_model(x)\n",
    "        mse_metric_0.update_state(y_0, pre_0)\n",
    "        mse_metric_1.update_state(y_1, pre_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7fe3104811d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fe3104813c8>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fe3104816d8>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7fe31047d860>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fe31047d780>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fe31049fc50>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fe3104a3e10>,\n",
       " <tensorflow.python.keras.layers.core.Activation at 0x7fe3104a3f28>,\n",
       " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fe44ffda278>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19min 48s, sys: 1min 42s, total: 21min 30s\n",
      "Wall time: 1min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "# save original weights\n",
    "og_conv_weights = sub_model.layers[1].get_weights()\n",
    "og_bn_weights = sub_model.layers[2].get_weights()\n",
    "\n",
    "filter_scores = []\n",
    "#prune and score each filter by MSE error\n",
    "for filters in range(64):\n",
    "    \n",
    "    # get inital weights\n",
    "    conv_weights = sub_model.layers[1].get_weights()\n",
    "    bn_weights = sub_model.layers[2].get_weights()\n",
    "    \n",
    "    #prun conv layer\n",
    "    conv_weights[0][:,:,:,filters] = np.zeros(conv_weights[0][:,:,:,filters].shape)\n",
    "    conv_weights[1][filters] = 0\n",
    "    \n",
    "    #prune batch norm variables\n",
    "    for weight in bn_weights:\n",
    "        weight[filters] = 0\n",
    "    \n",
    "    # reassign into submodel\n",
    "    sub_model.layers[1].set_weights(conv_weights)\n",
    "    sub_model.layers[2].set_weights(bn_weights)\n",
    "    \n",
    "    #calculate MSE over dataset and show result\n",
    "    calculate_mse(mse_metric_0, mse_metric_1, mse, train_dataset, output_model, sub_model)\n",
    "    l2_norm = np.linalg.norm(og_conv_weights[0][:,:,:,filters])\n",
    "    result_0 = mse_metric_0.result().numpy()\n",
    "    result_1 = mse_metric_1.result().numpy()\n",
    "    filter_scores.append({'filter': filters, 'mse': result_0, 'mse_1': result_1, 'L2': l2_norm})\n",
    "    mse_metric_0.reset_states()\n",
    "    mse_metric_1.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../cifar10.h5')\n",
    "m = tf.keras.metrics.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x_batch, y_batch) in test_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Loss 1.6843411922454834, Accuracy: 0.9035999774932861'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "mse_sorted_filters = sorted(filter_scores, key=lambda x: x['mse'])\n",
    "mse_prune_filters = [x['filter'] for x in mse_sorted_filters[:math.floor(64*.50)]]\n",
    "\n",
    "mse_sorted_filters_1 = sorted(filter_scores, key=lambda x: x['mse_1'])\n",
    "mse_prune_filters_1 = [x['filter'] for x in mse_sorted_filters_1[:math.floor(64*.60)]]\n",
    "\n",
    "mse_sorted_filters_2 = sorted(filter_scores, key=lambda x: x['mse_1'] + x['mse'])\n",
    "mse_prune_filters_2 = [x['filter'] for x in mse_sorted_filters_2[:math.floor(64*.60)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_conv_weights = copy.deepcopy(og_conv_weights)\n",
    "mse_bn_weights = copy.deepcopy(og_bn_weights)\n",
    "for index in mse_prune_filters_2:\n",
    "    mse_conv_weights[0][:,:,:, index] = np.zeros(conv_weights[0][:,:,:,index].shape)\n",
    "    mse_conv_weights[1][index] = 0\n",
    "    \n",
    "    for weight in mse_bn_weights:\n",
    "        weight[index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.8693946599960327, Accuracy: 0.9563800096511841\n",
      "Loss 2.928887128829956, Accuracy: 0.8289999961853027\n"
     ]
    }
   ],
   "source": [
    "m.reset_states()\n",
    "acc.reset_states()\n",
    "\n",
    "model.layers[7].set_weights(mse_conv_weights)\n",
    "\n",
    "model.layers[8].set_weights(mse_bn_weights)\n",
    "\n",
    "for (x_batch, y_batch) in train_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)\n",
    "\n",
    "print(f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\")\n",
    "m.reset_states()\n",
    "acc.reset_states()\n",
    "\n",
    "for (x_batch, y_batch) in test_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)\n",
    "print(f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\")\n",
    "\n",
    "model.load_weights(\"../cifar10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "l2_sorted_filters = sorted(filter_scores, key=lambda x: x['L2'])\n",
    "l2_prune_filters = [x['filter'] for x in l2_sorted_filters[:math.floor(64*.60)]]\n",
    "\n",
    "l2_conv_weights = copy.deepcopy(og_conv_weights)\n",
    "l2_bn_weights = copy.deepcopy(og_bn_weights)\n",
    "for index in l2_prune_filters:\n",
    "    l2_conv_weights[0][:,:,:, index] = np.zeros(conv_weights[0][:,:,:,index].shape)\n",
    "    l2_conv_weights[1][index] = 0\n",
    "    \n",
    "    for weight in l2_bn_weights:\n",
    "        weight[index] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.1719799041748047, Accuracy: 0.993340015411377\n",
      "Loss 2.1451756954193115, Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "m.reset_states()\n",
    "acc.reset_states()\n",
    "\n",
    "model.layers[7].set_weights(l2_conv_weights)\n",
    "\n",
    "model.layers[8].set_weights(l2_bn_weights)\n",
    "\n",
    "for (x_batch, y_batch) in train_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)\n",
    "\n",
    "print(f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\")\n",
    "m.reset_states()\n",
    "acc.reset_states()\n",
    "\n",
    "for (x_batch, y_batch) in test_dataset:\n",
    "    logits = model(x_batch)\n",
    "    m.update_state(logits, y_batch)\n",
    "    \n",
    "    prediction = tf.argmax(logits, axis=1)\n",
    "    labels = tf.argmax(y_batch, axis=1)\n",
    "    acc.update_state(prediction, labels)\n",
    "print(f\"Loss {m.result().numpy()}, Accuracy: {acc.result()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
